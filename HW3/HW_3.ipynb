{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejZ2oE4GmdfF"
      },
      "source": [
        "# Homework 3: Language Models, Contextual Embedding and BERT\n",
        "\n",
        "In this homework, we will explore implementations of various language models we saw in lecture. We will explore BERT and measure perplexity. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj0EWMnOmgWw"
      },
      "source": [
        "##Set Up\n",
        "\n",
        "If you're opening this Notebook on colab, you will probably need to install Transformers. Make sure your version of Transformers is at least 4.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svbBi3YGmMJX",
        "outputId": "dbebe5fe-751a-4b4c-a727-26c36d929af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhnQ3PoVnH7D",
        "outputId": "819bbea0-b796-4d2d-ba87-44dd5050a5a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.26.1\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpEDSQsLnbLm"
      },
      "source": [
        "IMPORTANT: For this assignment, GPU is not necessary. The following code block should show \"Running on cpu\". \n",
        "Go to Runtime > Change runtime type > Hardware accelerator > None if otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7osYx6Hm0vh",
        "outputId": "5614e11a-3576-4ac1-ba7c-d1fef04d69e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on {}\".format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ4XlcOvIHgl"
      },
      "source": [
        "# Masking\n",
        "\n",
        "One of the core ideas to wrap your head around with transformer-based language models (and PyTorch) is the concept of *masking*---preventing a model from seeing specific tokens in the input during training.\n",
        "\n",
        "* BERT training relies on the concept of *masked language modeling*: masking a random set of input tokens in a sequence and attempting to predict them.  Remember that BERT is *bidirectional*, so that it can use all of the other non-masked tokens in a sentence to make that prediction.\n",
        "\n",
        "* The GPT class of models acts as a traditional left-to-right language model (sometimes called a \"causal\" LM) .  This family also uses self-attention based transformers---but, when making a prediction for the word $w_i$ at position $i$, it can only use information about words $w_1, \\ldots, w_{i-1}$ to do so.  All of the other tokens following position $i-1$ must be *masked* (hidden from view).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi0LW-vKZnIH"
      },
      "source": [
        "Think about a mask as a matrix that's applied to every input $w$ when generating an output $o$ that determines whether an given $o_i$ is allowed to access each token in $w$.  For example, when passing a three-word input sequence through a transformer (to yield a three-word output sequence), a mask is a $3 \\times 3$ matrix where the cells are essentially  answering the following questions:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "o_1 \\; \\textrm{hide} \\; w_1\\textrm{?} & o_1 \\; \\textrm{hide} \\; w_2\\textrm{?} & o_1 \\; \\textrm{hide} \\; w_3\\textrm{?} \\\\\n",
        "o_2 \\; \\textrm{hide} \\; w_1\\textrm{?} & o_2 \\; \\textrm{hide} \\; w_2\\textrm{?} & o_2 \\; \\textrm{hide} \\; w_3\\textrm{?} \\\\\n",
        "o_3 \\; \\textrm{hide} \\; w_1\\textrm{?} & o_3 \\; \\textrm{hide} \\; w_2\\textrm{?} & o_3 \\; \\textrm{hide} \\; w_3\\textrm{?} \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "In the masks we will consider below, 1 denotes that a position should be hidden; 0 denotes that it should be visible. Consider this mask:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "0 & 1 & 1 \\\\\n",
        "1 & 0 & 1 \\\\\n",
        "1 & 1 & 0\n",
        "\\end{bmatrix}\n",
        "\n",
        "And consider this sequence:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "\\textrm{John} & \\textrm{likes}  & \\textrm{dogs}  \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "When applying this mask to that sequence, we're saying that when we're generating the output for $o_1$ (*John*), we can only consider $w_1$ as an input (*John*).  Likewise, when we generate the output for $o_2$ (*likes*), we can only consider $w_2$ as an input (*likes*), and so on.  (This is a terrible mask!  But illustrates what function a mask performs.)\n",
        "\n",
        "The following code illustrates how this works for that particular mask.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ea2E6zlMZkzO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def visualize_masking(sequences, mask):\n",
        "  print(mask)\n",
        "  for sequence in sequences:\n",
        "    for i in range(len(sequence)):\n",
        "      visible=[]\n",
        "      for j in range(len(sequence)):\n",
        "        if mask[i][j]==0:\n",
        "          visible.append(sequence[j])\n",
        "      print(\"for word %s, the following tokens are visible: %s\" % (sequence[i], visible))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xROu5KmvKMua",
        "outputId": "42417090-04ed-4904-f9ea-b8186e118f32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]]\n",
            "for word This, the following tokens are visible: ['This']\n",
            "for word is, the following tokens are visible: ['is']\n",
            "for word a, the following tokens are visible: ['a']\n",
            "for word sentence, the following tokens are visible: ['sentence']\n",
            "for word that, the following tokens are visible: ['that']\n",
            "for word has, the following tokens are visible: ['has']\n",
            "for word exactly, the following tokens are visible: ['exactly']\n",
            "for word ten, the following tokens are visible: ['ten']\n",
            "for word tokens, the following tokens are visible: ['tokens']\n",
            "for word ., the following tokens are visible: ['.']\n",
            "\n",
            "for word Here's, the following tokens are visible: [\"Here's\"]\n",
            "for word another, the following tokens are visible: ['another']\n",
            "for word sequence, the following tokens are visible: ['sequence']\n",
            "for word with, the following tokens are visible: ['with']\n",
            "for word 10, the following tokens are visible: ['10']\n",
            "for word words, the following tokens are visible: ['words']\n",
            "for word like, the following tokens are visible: ['like']\n",
            "for word the, the following tokens are visible: ['the']\n",
            "for word last, the following tokens are visible: ['last']\n",
            "for word ., the following tokens are visible: ['.']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sequences=[[\"This\", \"is\", \"a\", \"sentence\", \"that\", \"has\", \"exactly\", \"ten\", \"tokens\", \".\"], [\"Here's\", \"another\", \"sequence\", \"with\", \"10\", \"words\", \"like\", \"the\", \"last\", \".\"]]\t\n",
        "\n",
        "seq_length=len(sequences[0])\n",
        "\n",
        "test_mask=np.ones((seq_length,seq_length))\n",
        "for i in range(seq_length):\n",
        "  test_mask[i,i]=0\n",
        "\n",
        "visualize_masking(sequences, test_mask)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKv3h625eMo3"
      },
      "source": [
        "##Q1.  \n",
        "As we discussed in class, BERT masks a random set of words in the input and attempts to reconstruct those words as output.  Create a mask that randomly masks token positions 2 and 7 (for an input sequence length of 10 tokens, with 0 being the position of the first token).  For an input sequence of 10 tokens, you should generate output representations for all 10 tokens (i.e., $[o_1, \\ldots, o_{10}]$ in the notation above, but each representation must ignore the same 2 input tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_31mkhIke-YI"
      },
      "outputs": [],
      "source": [
        "def create_bert_mask(seq_length):\n",
        "  mask=np.zeros((seq_length,seq_length))\n",
        "  # implement BERT mask here\n",
        "  \n",
        "  # BEGIN SOLUTION\n",
        "  for row in range(seq_length):\n",
        "    mask[row, 2] = 1\n",
        "    mask[row, 7] = 1\n",
        "\n",
        "  # END SOLUTION\n",
        "\n",
        "  return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrFviJWfQ1Rz"
      },
      "outputs": [],
      "source": [
        "# create_bert_mask(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKjasreUfLww"
      },
      "source": [
        "##Q2\n",
        "A left-to-right language model (such as GPT) can only use information from input words $[w_1, \\ldots, w_{i}]$ when generating the representation for output $o_i$.  Encode this as a mask as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svKuQyQff4fK"
      },
      "outputs": [],
      "source": [
        "def create_causal_mask(seq_length):\n",
        "  mask=np.zeros((seq_length,seq_length))\n",
        "  # implement causal mask here\n",
        "\n",
        "  # BEGIN SOLUTION\n",
        "  for row in range(seq_length):\n",
        "    mask[row, row+1:] = 1\n",
        "\n",
        "  # END SOLUTION\n",
        "  \n",
        "  return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V45SkgcUScs6",
        "outputId": "8f4d33d0-8064-413a-ade9-0fabbc517885"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "create_causal_mask(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXRkI2DZnwvB"
      },
      "source": [
        "Now let's go ahead and embed these masks within a model.  First, we'll load some textual data (from Austen's *Pride and Prejudice*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgTwRcqDn6ni",
        "outputId": "3d0a30ca-54a7-4075-e259-0c627b59c1ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-02-22 03:41:52--  https://www.gutenberg.org/files/1342/1342-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 772145 (754K) [text/plain]\n",
            "Saving to: ‘1342-0.txt’\n",
            "\n",
            "1342-0.txt          100%[===================>] 754.05K  3.88MB/s    in 0.2s    \n",
            "\n",
            "2023-02-22 03:41:52 (3.88 MB/s) - ‘1342-0.txt’ saved [772145/772145]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.gutenberg.org/files/1342/1342-0.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v2EJeEBobD8"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8hjeySdojyr",
        "outputId": "8d09f160-f770-4641-c1f3-8cdc4f325817"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jre3sSWgvnwi"
      },
      "source": [
        "Let's read in the data and tokenize it; for this homework, we'll only work with the first 10,000 tokens of that book; we'll keep only the most frequent 1,000 word types (all other tokens will be mapped to an [UNK] token)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kzw9IcFoDZq"
      },
      "outputs": [],
      "source": [
        "def read_data(filename):\n",
        "  with open(filename) as file:\n",
        "    data=file.read().lower()\n",
        "    first10K=' '.join(data.split(\" \")[:10000])\n",
        "    toks=nltk.word_tokenize(first10K)[:10000]\n",
        "    vocab={\"[PAD]\":0, \"[UNK]\":1}\n",
        "    counts=Counter()\n",
        "    for tok in toks:\n",
        "      counts[tok]+=1\n",
        "    for v, _ in counts.most_common(1000):\n",
        "      vocab[v]=len(vocab)\n",
        "    tokids=[]\n",
        "    for tok in toks:\n",
        "      tokid=1\n",
        "      if tok in vocab:\n",
        "        tokid=vocab[tok]\n",
        "      \n",
        "      tokids.append(tokid)\n",
        "\n",
        "    return tokids, vocab  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r_XHBZPv4kD"
      },
      "source": [
        "Now let's specify our model in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p74L0s1DsKa2"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "class MaskedLM(nn.Module):\n",
        "    def __init__(self, vocab, mask, d_model=512):       \n",
        "        super().__init__()\n",
        "        self.vocab=vocab\n",
        "        self.mask=mask\n",
        "        vocab_size=len(vocab)\n",
        "        self.embeddings=nn.Embedding(1002,512)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead=8, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "        self.linear=torch.nn.Linear(d_model, vocab_size)\n",
        "        self.rev_vocab={vocab[k]:k for k in vocab}\n",
        "\n",
        "    def forward(self, input): \n",
        "        # first we pass the input word IDS through an embedding layer to get embeddings for them\n",
        "        input=self.embeddings(input)\n",
        "        # then we pass those embeddings through a transformer to get contextual representations, masking the input where appropriate\n",
        "        out = self.transformer_encoder.forward(input, mask=self.mask)        \n",
        "        # finally we pass those embeddings through a linear layer to transform it into the output space (the size of our vocabulary)\n",
        "        h=self.linear(out)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njdPnGVQsQOl"
      },
      "outputs": [],
      "source": [
        "def get_batches(xs, ys, batch_size=32):\n",
        "    batch_x=[]\n",
        "    batch_y=[]\n",
        "    for i in range(0, len(xs), batch_size):\n",
        "        batch_x.append(torch.LongTensor(xs[i:i+batch_size]).to(device))\n",
        "        batch_y.append(torch.LongTensor(ys[i:i+batch_size]).to(device))\n",
        "    return batch_x, batch_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm6tLzHmtAfN"
      },
      "outputs": [],
      "source": [
        "tokids, vocab=read_data(\"1342-0.txt\")  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9822mHI3sR1Q"
      },
      "outputs": [],
      "source": [
        "def train(mask, data_function, tokids, vocab):\n",
        "\n",
        "    mask=torch.BoolTensor(mask).to(device)\n",
        "\n",
        "    num_labels=len(vocab)\n",
        "    model=MaskedLM(vocab, mask).to(device)\n",
        "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "    cross_entropy=nn.CrossEntropyLoss()\n",
        "    losses=[]\n",
        "\n",
        "    xs, ys=data_function(tokids)\n",
        "\n",
        "    batch_x, batch_y=get_batches(xs, ys)\n",
        "\n",
        "    for epoch in range(1):\n",
        "        model.train()\n",
        "        \n",
        "        for x, y in list(zip(batch_x, batch_y)):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_pred=model.forward(x)\n",
        "            loss=cross_entropy(y_pred.view(-1, num_labels), y.view(-1))\n",
        "            losses.append(loss.item())\n",
        "            print(loss)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CXKNwLBwBG6"
      },
      "source": [
        "Our model and training process are now all defined; all that remains is to pass our inputs and outputs through it to train.  Your job here is to create the correct inputs (x) and outputs (y) to train a left-to-right (causal) language model.\n",
        "\n",
        "##Q3\n",
        "Write a function that takes in a sequence of token ids $[w_1, \\ldots, w_n]$ and segments it into 8-token chunks -- e.g., $x_1=[w_1, \\ldots, w_8]$, $x_2=[w_9, \\ldots, w_{16}]$, etc.  For each $x_i$, also create its corresponding $y_i$.  Given this language modeling specification, each $y_i$ should also contain 8 values (for each token in $x_i$).  Keep in mind this is a left-to-right causal language model; your job is to figure out the values of y that respects this design.  At token position $i$, when a model has access to $[w_1, \\ldots, w_i]$, which is the true $y_i$ for that position? Each element in $y$ should be a word ID (i.e., an integer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIO8M9AeszAi"
      },
      "outputs": [],
      "source": [
        "def get_causal_xy(data, max_len=8):\n",
        "    xs=[]\n",
        "    ys=[]\n",
        "    \n",
        "    # BEGIN SOLUTION\n",
        "    xs = [data[i:i + max_len] for i in range(0, len(data), max_len)]\n",
        "    ys = [data[i:i + max_len] for i in range(1, len(data), max_len)]\n",
        "\n",
        "    if len(xs[-1]) != 8:\n",
        "      xs.pop(-1)\n",
        "      ys.pop(-1)\n",
        "    # END SOLUTION\n",
        "\n",
        "    return xs, ys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO4WOusZsWsC",
        "outputId": "08c1923a-67b3-4c30-a23e-17108deaa99b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(7.0404, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.3965, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.4171, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.2213, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.3118, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.0549, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.0128, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.0678, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.2065, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.5382, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.2874, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.6702, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.5433, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.3102, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.0360, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1285, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1780, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.8283, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.6504, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.2787, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.3913, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.3472, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.7526, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.9070, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "seq_length=8\n",
        "\n",
        "train(create_causal_mask(seq_length=seq_length), get_causal_xy, tokids, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2P8GU1VsKLn"
      },
      "source": [
        "##Q4 (Write-up)  \n",
        "In this model, as implemented, does the following equivalence hold?\n",
        "\n",
        "$$\n",
        "P(y_4 \\mid w_1 = \\textrm{go}, w_2=\\textrm{ahead}, w_3=\\textrm{make}, w_4=\\textrm{my})= P(y_4 \\mid w_1 = \\textrm{ahead}, w_2=\\textrm{my}, w_3=\\textrm{make}, w_4=\\textrm{go})\n",
        "$$\n",
        "\n",
        "Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pZpVrqchSAI"
      },
      "source": [
        "# Perplexity\n",
        "To evaluate how good our language model is, we use a metric called perplexity. The perplexity of a language model (PP) on a test set is the inverse probability of the test set, normalized by the number of words. Let $W = w_{1}w_{2}\\dots w_{N}$. Then,\n",
        "\n",
        "$$PP(W) = \\sqrt[N]{\\prod_{i = 1}^{N}\\frac{1}{P(w_{i}|w_{1}\\dots w_{i - 1})}}$$\n",
        "\n",
        "However, since these probabilities are often small, taking the inverse and multiplying can be numerically unstable, so we often first compute these values in the log domain and then convert back. So this equation looks like:\n",
        "\n",
        "$$\\ln PP(W) = \\frac{1}{N} \\sum_{i = 1}^{N} -\\ln P(w_{i}|w_{1}\\dots w_{i - 1})$$\n",
        "\n",
        "$$\\implies PP(W) = e^{\\frac{1}{N} \\sum_{i = 1}^{N} -\\ln P(w_{i}|w_{1}\\dots w_{i - 1})}$$\n",
        "\n",
        "Here we want to calculate the perplexity of [pretrained BERT model](https://huggingface.co/bert-base-uncased) on text from different sources. When calculating perplexity with BERT, we'll use a related measure of pseudo-perplexity, which allow us to condition on the bidirectional context (and not just the left context, as in standard perplexity):\n",
        "\n",
        "$$PP(W) = e^{\\frac{1}{N} \\sum_{i = 1}^{N} -\\ln P(w_{i} \\mid w_{1}\\dots w_{i - 1}, w_{i+1}, \\ldots, w_n)}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2MXSfzlsHp_"
      },
      "source": [
        "First, let's instantiate a BERT model, along with its WordPiece tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "XxyJaS1-cP3R",
        "outputId": "ceb88679-6241-4a30-fdb7-284b9cb007d1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b0ce03a8a6f4ab18bf3a97cceaacf6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e49acac514804ad6a7094d2933939c9f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21f2eaaf88ba4ddfad5128233dcb611f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5369db7cb57436fb414fddbb85efd8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d61cd08404c44991923e6ecad7d8a327",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "model_name = 'bert-base-uncased'\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model=model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLAWTU8sTqf"
      },
      "source": [
        "Let's see how the BERT tokenizer tokenizes a sentence into a sequence of WordPiece ids.  Note how BERT tokenization automatically wraps an input sentences with [CLS] and [SEP] tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxlOsB0NshJm",
        "outputId": "1477457a-c078-4c6e-9a02-ca7996eaad66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[ 101, 1037, 3899, 5565, 2006, 7733,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
            "tensor([[ 101, 1037, 3899, 5565, 2006, 7733,  102]])\n",
            "['[CLS]', 'a', 'dog', 'landed', 'on', 'mars', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "sentence = \"A dog landed on Mars\"\n",
        "tensor_input = tokenizer(sentence, return_tensors=\"pt\")\n",
        "print(tensor_input)\n",
        "tensor_input_ids = tensor_input[\"input_ids\"]\n",
        "print(tensor_input_ids)\n",
        "print(tokenizer.convert_ids_to_tokens(tensor_input_ids[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGu38_DB1vsB"
      },
      "source": [
        "Now let's see how we can calculate output probabilities using this model.  The output of each token position $i$ gives us $P(w_i \\mid w_1, \\ldots, w_n)$---the probability of the word at that position over our vocabulary, given *all* of the words in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvuNw52m15OU",
        "outputId": "a39f2e04-faa6-417f-acf6-b1030be15b98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 7, 30522])\n",
            "[CLS]\t101\t0.00000\n",
            "a\t1037\t0.99281\n",
            "dog\t3899\t0.99052\n",
            "landed\t5565\t0.99809\n",
            "on\t2006\t0.99874\n",
            "mars\t7733\t0.00133\n",
            "[SEP]\t102\t0.00000\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  output = model(tensor_input_ids)\n",
        "  logits = output.logits\n",
        "  # logits here are the unnormalized scores, so let's pass them through the softmax \n",
        "  # to get a probability distribution\n",
        "  softmax = torch.nn.functional.softmax(logits, dim = -1)\n",
        "  # for one input sequence, the shape of the resulting distribution is: \n",
        "  # 1 x [length of input, in WordPiece tokens] x (the size of the BERT vocabulary)\n",
        "  print(softmax.shape) # [1, 7, 30522]\n",
        "  input_ints=tensor_input_ids.numpy()[0]\n",
        "  # Let's print the probability of the true inputs\n",
        "  wp_tokens=tokenizer.convert_ids_to_tokens(input_ints)\n",
        "  for i in range(len(input_ints)):\n",
        "    prob=softmax[0][i][input_ints[i]].numpy()\n",
        "    print(\"%s\\t%s\\t%.5f\" % (wp_tokens[i], input_ints[i], prob))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F90uSrL_1vot"
      },
      "source": [
        "Note that $w_i$ is in the range $[w_1, \\ldots, w_n]$ -- clearly the probability of a word is going to be high when we can observe it in the input! Let's do some masking to calculate $P(w_i \\mid w_1, \\ldots w_{i-1}, w_{i+1}, w_n)$.  Now annoyingly, BERT's `attention_mask` function only works for padding tokens; to mask input tokens, we need to intervene in the input and replace a WordPiece token that we're predicting with a special [MASK] token (BERT tokenizer word id `103`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77EdDCtk3MSa",
        "outputId": "49f8b7a6-57f1-4bb6-8963-92de753346a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The second word here now is [MASK] token ID '103':  tensor([[ 101,  103, 3899, 5565, 2006, 7733,  102]])\n",
            "a\t1037\t0.13965\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "with torch.no_grad():\n",
        "  # let's make a copy of the original word ids so we can mask one of the tokens\n",
        "  masked_input_ids=copy.deepcopy(tensor_input_ids)\n",
        "  # we'll mask the second word\n",
        "  masked_input_ids[0][1]=tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
        "\n",
        "  print(\"The second word here now is [MASK] token ID '103': \", masked_input_ids)\n",
        "\n",
        "  # now let's run that through BERT in the same way we did before\n",
        "  output = model(masked_input_ids)\n",
        "  logits = output.logits\n",
        "\n",
        "  softmax = torch.nn.functional.softmax(logits, dim = -1)\n",
        "  input_ints=tensor_input_ids.numpy()[0]\n",
        "\n",
        "  wp_tokens=tokenizer.convert_ids_to_tokens(input_ints)\n",
        "  i=1\n",
        "  prob=softmax[0][i][input_ints[i]].numpy()\n",
        "  print(\"%s\\t%s\\t%.5f\" % (wp_tokens[i], input_ints[i], prob))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSdj7onm1vln"
      },
      "source": [
        "You can see the probability of \"a\" as the second token has gone down to 0.13965 when we mask it.  This is the $P(w_1 =\\textrm{a} \\mid w_0, w_2, \\ldots, w_n)$.  At this point you should have everything you need to calculate the BERT pseudo-perplexity of an input sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA_eHLic_OfM"
      },
      "source": [
        "##Q5\n",
        "Implement the pseudo-perplexity measure described above, calculating the perplexity for a given model, tokenizer, and sentence. \n",
        "\n",
        "The function calculates the average probability of each token in the sentence given all the other tokens. We need to predict the probability of each word in a sentence by masking the one word to predict. Note that you should not include the probabilities of the [CLS] and [SEP] tokens in your perplexity equation -- those tokens are not part of the original test sentence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "GDZ9Qe8L-uUt"
      },
      "outputs": [],
      "source": [
        "# This function calculates the perplexity of a language model, given a sentence and its corresponding tokenizer\n",
        "\n",
        "# Inputs:\n",
        "# model: language model being used to calculate the perplexity\n",
        "# tokenizer: tokenizer that is used to preprocess the input sentence\n",
        "# sentence: input sentence string for which perplexity is to be calculated\n",
        "\n",
        "# Outputs:\n",
        "# returns perplexity of the input sentence\n",
        "\n",
        "\n",
        "def perplexity(model, tokenizer, sentence):\n",
        "  # hints: you'll need to:\n",
        "  # encode the input sentence using the tokenizer\n",
        "  # for each WordPiece token in the sentence (except [CLS] and [SEP]), mask that single token and   \n",
        "  # calculate the probability of that true word at the masked position\n",
        "  # don't calculate perplexity for the [CLS] and [SEP] tokens (which are not part of the original test sentence).\n",
        "  perplexity = None\n",
        "\n",
        "  # BEGIN SOLUTION\n",
        "  tensor_input = tokenizer(sentence, return_tensors=\"pt\")\n",
        "  tensor_input_ids = tensor_input[\"input_ids\"] \n",
        "\n",
        "  input_ints=tensor_input_ids.numpy()[0]\n",
        "  wp_tokens=tokenizer.convert_ids_to_tokens(input_ints)\n",
        "  N = len(input_ints) - 2\n",
        "  probabilities = np.zeros(shape=(1,N))\n",
        "  i = 1 # start at 1 to exclude padding token CLS\n",
        "\n",
        "  for i in range(i, len(input_ints) - 1): # end at len - 1 to exclude padding token SEP\n",
        "    with torch.no_grad():\n",
        "      # make copy\n",
        "      masked_input_ids=copy.deepcopy(tensor_input_ids)\n",
        "      # mask word i\n",
        "      masked_input_ids[0][i]=tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
        "      # print(f\"The #{i} word here now is [MASK] token ID '103': \", masked_input_ids)\n",
        "      # run through BERT\n",
        "      output = model(masked_input_ids)\n",
        "      logits = output.logits\n",
        "      # calculate softmax'd probas\n",
        "      softmax = torch.nn.functional.softmax(logits, dim = -1)\n",
        "      prob=softmax[0][i][input_ints[i]].numpy()\n",
        "      # print(\"%s\\t%s\\t%.5f\" % (wp_tokens[i], input_ints[i], prob))\n",
        "      probabilities[0, i-1] = prob # append probabilities to array\n",
        "\n",
        "  # calculate perplexity\n",
        "  perplexity = np.exp((1/N) * np.sum(-1 * np.log(probabilities)))\n",
        "  # END SOLUTION\n",
        "  return perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tN7HVo_oIWXF",
        "outputId": "bf929916-d636-44b8-edd8-547765767555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0596669784644086\n"
          ]
        }
      ],
      "source": [
        "print(perplexity(sentence='London is the capital of the United Kingdom.', model=model, tokenizer=tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsHLgqH0-6_t"
      },
      "source": [
        "# No credit.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5eR8EF12v1s"
      },
      "source": [
        "We provide [texts](https://people.ischool.berkeley.edu/~dbamman/text_from_different_sources.txt) from 4 different sources ([Wikipedia](https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots), [Yelp](https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset), [Fiction](https://github.com/dbamman/litbank), [Twitter](https://github.com/dbamman/anlp21/blob/main/data/potus_tweets.json)) collected from open-source datasets. Each category has 125 entries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnkdWOu1BGjP",
        "outputId": "6b111c9a-50d2-4a76-b194-76cbfbbc2bfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-02-22 03:42:34--  https://people.ischool.berkeley.edu/~dbamman/text_from_different_sources.txt\n",
            "Resolving people.ischool.berkeley.edu (people.ischool.berkeley.edu)... 128.32.78.16\n",
            "Connecting to people.ischool.berkeley.edu (people.ischool.berkeley.edu)|128.32.78.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 61117 (60K) [text/plain]\n",
            "Saving to: ‘text_from_different_sources.txt’\n",
            "\n",
            "text_from_different 100%[===================>]  59.68K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-02-22 03:42:35 (625 KB/s) - ‘text_from_different_sources.txt’ saved [61117/61117]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://people.ischool.berkeley.edu/~dbamman/text_from_different_sources.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpCpGQpG4OJZ",
        "outputId": "8ccb76c6-bba0-45b5-c211-e9335bcf8d30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wikipedia 125\n",
            "Yelp 125\n",
            "Fiction 125\n",
            "Twitter 125\n"
          ]
        }
      ],
      "source": [
        "text_by_genre={}\n",
        "with open('text_from_different_sources.txt') as file:\n",
        "  file.readline()\n",
        "  for line in file:\n",
        "    cols=line.rstrip().split(\"\\t\")\n",
        "    genre=cols[0]\n",
        "    text=cols[1]\n",
        "\n",
        "    if genre not in text_by_genre:\n",
        "      text_by_genre[genre]=[]\n",
        "    text_by_genre[genre].append(text)\n",
        "\n",
        "for genre in text_by_genre:\n",
        "  print(genre, len(text_by_genre[genre]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtE7Umzy9xRj"
      },
      "source": [
        "Calculate perplexity on each genre over all of the words present within it; each line contains exactly one sentence for each genre.\n",
        "\n",
        "The output perplexity_by_genre = {} is a dictionary mapping genre to a list of perplexities for each sentence in that genre. For computational purpose, we only take the first 25 sentences as an example (still this can take up to 10 minutes to run), feel free to change 25 to smaller numbers.\n",
        "\n",
        "e.g. perplexity_by_genre['Wikipedia'] should be a list of 25 perplexities (one for each Wikipedia row in the input file)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "09NqLCO0523E"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def calculate_perplexity_by_genre(text_by_genre):\n",
        "  perplexity_by_genre = {}\n",
        "  for genre in text_by_genre:\n",
        "    perplexity_by_genre[genre] = []\n",
        "    for text in text_by_genre[genre][:25]: # change 25 to smaller numbers if necessary\n",
        "      p = perplexity(sentence=text, model=model, tokenizer=tokenizer)\n",
        "      perplexity_by_genre[genre].append(p)\n",
        "  return perplexity_by_genre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7sXsSGbd730C"
      },
      "outputs": [],
      "source": [
        "# running this might take up to 10 minutes\n",
        "perplexity_by_genre = calculate_perplexity_by_genre(text_by_genre)\n",
        "for genre in perplexity_by_genre:\n",
        "  print(\"Genre:\",genre,\", mean perplexity:\",np.mean(perplexity_by_genre[genre]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWk0jHUM8UFP"
      },
      "source": [
        "##Question: \n",
        "What do you think are the reasons for the wide variation in perplexity of different categories of corpus? (hint: think about the training data of the pre-trained BERT model)\n",
        "\n",
        "\n",
        "Which of these is a true language model, and why?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}